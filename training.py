import streamlit as st
import pandas as pd
import numpy as np
from data_preprocessing import (
    load_data, handle_missing_values, normalize_data, enhanced_data_preview,
    detect_and_handle_outliers, advanced_feature_engineering, show_preprocessing_summary
)
from feature_selection import feature_selection_page
from model_training import (
    get_available_models, train_model, enhanced_cross_validation, 
    hyperparameter_tuning, compare_models, get_model_recommendations
)
from model_evaluation import evaluate_model, create_learning_curves
from utils import save_model_for_download, generate_report, export_model_summary
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import os
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')
from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler
from sklearn.preprocessing import LabelEncoder

def training_page():
    col1, col2, col3 = st.columns([1, 6, 1])
    with col1:
        if st.button("ğŸ  Home", key="training_back_home", type="secondary"):
            st.session_state.current_page = 'home'
            st.rerun()
    
    st.title("ğŸ“Š Advanced Training Data Platform")
    st.markdown("### *Enhanced ML Training dengan Automated Features*")
    
    if 'training_step' not in st.session_state:
        st.session_state.training_step = 1
    
    progress_steps = [
        {"name": "Upload Data", "icon": "ğŸ“¤", "desc": "Smart data upload & preview"},
        {"name": "Preprocessing", "icon": "âš™ï¸", "desc": "Advanced cleaning & engineering"}, 
        {"name": "Feature Selection", "icon": "ğŸ¯", "desc": "Automated & manual selection"},
        {"name": "Model Training", "icon": "ğŸ¤–", "desc": "Training & hyperparameter tuning"},
        {"name": "Evaluation", "icon": "ğŸ“ˆ", "desc": "Advanced metrics & visualization"}
    ]
    current_step = st.session_state.training_step
    
    st.markdown("### ğŸš€ Training Workflow Progress")
    
    progress_cols = st.columns(5)
    for i, step in enumerate(progress_steps, 1):
        with progress_cols[i-1]:
            if i < current_step:
                st.markdown(f"""
                <div style="text-align: center; padding: 15px; background-color: #d4edda; border-radius: 10px; border-left: 4px solid #28a745;">
                    <div style="font-size: 24px;">{step['icon']}</div>
                    <div style="font-weight: bold; color: #155724;">âœ… {step['name']}</div>
                    <div style="font-size: 12px; color: #155724;">{step['desc']}</div>
                </div>
                """, unsafe_allow_html=True)
            elif i == current_step:
                st.markdown(f"""
                <div style="text-align: center; padding: 15px; background-color: #fff3cd; border-radius: 10px; border-left: 4px solid #ffc107;">
                    <div style="font-size: 24px;">{step['icon']}</div>
                    <div style="font-weight: bold; color: #856404;">ğŸ“ {step['name']}</div>
                    <div style="font-size: 12px; color: #856404;">{step['desc']}</div>
                </div>
                """, unsafe_allow_html=True)
            else:
                st.markdown(f"""
                <div style="text-align: center; padding: 15px; background-color: #f8f9fa; border-radius: 10px; border-left: 4px solid #6c757d;">
                    <div style="font-size: 24px; opacity: 0.5;">{step['icon']}</div>
                    <div style="font-weight: bold; color: #6c757d;">â³ {step['name']}</div>
                    <div style="font-size: 12px; color: #6c757d;">{step['desc']}</div>
                </div>
                """, unsafe_allow_html=True)
    
    st.markdown("---")
    
    if current_step == 1:
        enhanced_data_upload_step()
    elif current_step == 2:
        enhanced_data_preprocessing_step()
    elif current_step == 3:
        enhanced_feature_selection_step()
    elif current_step == 4:
        enhanced_model_training_step()
    elif current_step == 5:
        enhanced_model_evaluation_step()

def enhanced_data_upload_step():
    """Enhanced data upload with automatic preview and column selection"""
    st.header("1. ğŸ“¤ Advanced Data Upload")
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        uploaded_files = st.file_uploader(
            "ğŸ“ Select CSV or Excel files",
            type=['csv', 'xlsx', 'xls'],
            accept_multiple_files=True,
            help="Upload multiple files that will be intelligently combined"
        )
    
    with col2:
        st.markdown("### ğŸ’¡ Upload Tips")
        st.info("""
        **Supported formats:**
        - CSV (auto-delimiter detection)
        - Excel (.xlsx, .xls)
        
        **Smart features:**
        - Auto data type detection
        - Column preview & selection
        - Duplicate detection
        - Format validation
        """)
    
    if uploaded_files:
        st.success(f"âœ… {len(uploaded_files)} file(s) uploaded successfully")
        
        st.subheader("ğŸ“Š File Analysis")
        
        file_info = []
        total_size = 0
        
        for file in uploaded_files:
            size_kb = file.size / 1024
            total_size += size_kb
            file_info.append({
                'File Name': file.name,
                'Size (KB)': f"{size_kb:.1f}",
                'Type': file.type if hasattr(file, 'type') else file.name.split('.')[-1].upper()
            })
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Total Files", len(uploaded_files))
        with col2:
            st.metric("Total Size", f"{total_size:.1f} KB")
        with col3:
            st.metric("File Types", len(set([info['Type'] for info in file_info])))
        
        st.dataframe(pd.DataFrame(file_info), use_container_width=True)
        
        if st.button("ğŸ” Load & Preview Data", type="primary", use_container_width=True):
            with st.spinner("Loading and analyzing data..."):
                try:
                    data = load_data(uploaded_files, header_row=0, headers_not_in_first_row=False)
                    
                    if data is not None:
                        st.session_state.raw_data = data
                        st.session_state.original_raw_data = data.copy()  # Store original
                        st.session_state.preview_data = data  
                        
                        st.success("âœ… Data loaded successfully!")
                        display_data_preview_and_analysis(data)
                        
                except Exception as e:
                    st.error(f"âŒ Error loading data: {str(e)}")
                    st.error("ğŸ’¡ Please check your file format and configuration.")
        
        if hasattr(st.session_state, 'preview_data') and st.session_state.preview_data is not None:
            st.markdown("---")
            st.subheader("âš™ï¸ Advanced Configuration")
            st.info("Configure advanced options and apply them to update the data")
            
            col1, col2 = st.columns(2)
            
            with col1:
                use_custom_header = st.checkbox(
                    "Headers (row) selection",
                    help="Pilih row untuk header dari kolom data yang ingin digunakan"
                )
                
                header_row = 0
                if use_custom_header:
                    header_row = st.number_input(
                        "Header row (0-indexed):",
                        min_value=0,
                        max_value=10,
                        value=0
                    )
            
            with col2:
                use_specific_columns = st.checkbox(
                    "Column  sellection",
                    help="Pilih kolom spesifik yang ingin digunakan untuk pelatihan"
                )
            
            show_config_form = use_custom_header or use_specific_columns
            selected_columns = None
            
            if show_config_form:
                st.markdown("#### ğŸ”§ Configuration Settings")
                
                if use_specific_columns:
                    current_data = st.session_state.preview_data
                    all_columns = current_data.columns.tolist()
                    
                    st.write("**Column Selection:**")
                    selected_columns = st.multiselect(
                        "Select columns to use:",
                        all_columns,
                        default=all_columns,
                        help="Choose which columns to include in the dataset"
                    )
                
                if st.button("ğŸ”§ Apply Configuration & Update Data", type="secondary", use_container_width=True):
                    with st.spinner("Applying configuration and reloading data..."):
                        try:
                            updated_data = load_data(
                                uploaded_files, 
                                header_row=header_row, 
                                headers_not_in_first_row=use_custom_header
                            )
                            
                            if updated_data is not None:
                                if use_specific_columns and selected_columns:
                                    updated_data = updated_data[selected_columns]
                                
                                st.session_state.raw_data = updated_data
                                st.session_state.original_raw_data = updated_data.copy()  # Update original
                                st.session_state.preview_data = updated_data
                                
                                st.success("âœ… Configuration applied successfully!")
                                display_data_preview_and_analysis(updated_data)
                                
                        except Exception as e:
                            st.error(f"âŒ Error applying configuration: {str(e)}")

    if hasattr(st.session_state, 'raw_data') and st.session_state.raw_data is not None:
        st.markdown("---")
        st.info("âœ… Data successfully loaded and analyzed! Ready for preprocessing.")
        
        col1, col2 = st.columns([1, 1])
        with col1:
            if st.button("â™»ï¸ Reload Data", key="reload_data"):
                if 'raw_data' in st.session_state:
                    del st.session_state.raw_data
                if 'original_raw_data' in st.session_state:
                    del st.session_state.original_raw_data
                if 'preview_data' in st.session_state:
                    del st.session_state.preview_data
                st.rerun()
        
        with col2:
            if st.button("â¡ï¸ Continue to Preprocessing", key="proceed_to_preprocessing", type="primary"):
                st.session_state.training_step = 2
                st.rerun()

def display_data_preview_and_analysis(data):
    """Display data preview and analysis in a organized way"""
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.success(f"**Rows:** {data.shape[0]:,}")
    with col2:
        st.success(f"**Columns:** {data.shape[1]}")
    with col3:
        st.success(f"**Memory:** {data.memory_usage(deep=True).sum() / 1024**2:.1f} MB")
    with col4:
        missing_pct = (data.isnull().sum().sum() / (data.shape[0] * data.shape[1])) * 100
        st.success(f"**Missing:** {missing_pct:.1f}%")

    st.subheader("ğŸ“‹ Data Preview & Analysis")
    enhanced_data_preview(data)

def enhanced_data_preprocessing_step():
    """FIXED: Enhanced preprocessing with modular optional steps and data reset functionality"""
    st.header("2. âš™ï¸ Advanced Data Preprocessing")
    
    if not hasattr(st.session_state, 'raw_data') or st.session_state.raw_data is None:
        st.warning("âš ï¸ No data loaded. Please return to step 1.")
        if st.button("â¬…ï¸ Back to Upload", key="back_to_upload_from_preprocessing"):
            st.session_state.training_step = 1
            st.rerun()
        return
    
    data = st.session_state.raw_data

    # Initialize processed_data if not exists
    if 'processed_data' not in st.session_state or st.session_state.processed_data is None:
        st.session_state.processed_data = data.copy()

    st.subheader("ğŸ“Š Original Data Preview")
    st.dataframe(data.head(), use_container_width=True)

    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("Rows", data.shape[0])
    with col2:
        st.metric("Columns", data.shape[1])
    with col3:
        string_cols = data.select_dtypes(include=['object']).shape[1]
        st.metric("String Columns", string_cols)
    with col4:
        missing_pct = (data.isnull().sum().sum() / (data.shape[0] * data.shape[1])) * 100
        st.metric("Missing %", f"{missing_pct:.1f}%")

    string_columns = data.select_dtypes(include=['object']).columns.tolist()
    if string_columns:
        st.info(f"ğŸ·ï¸ **String columns detected**: {', '.join(string_columns[:5])}{'...' if len(string_columns) > 5 else ''}")
        st.write("These columns will be automatically encoded if used as target or features.")
    
    st.markdown("---")

    st.subheader("ğŸ”§ Preprocessing Pipeline Configuration")
    
    # Step 1: Missing Values
    step1_col1, step1_col2 = st.columns([1, 3])
    with step1_col1:
        enable_missing = st.checkbox("âœ… Apply Missing Value Handling", key="enable_missing")
    with step1_col2:
        if enable_missing:
            st.session_state.show_missing_config = True
    
    if enable_missing and st.session_state.get('show_missing_config', False):
        with st.expander("ğŸ§¹ Missing Values Configuration", expanded=True):
            missing_config = configure_missing_values_step(st.session_state.processed_data)
            if st.button("â–¶ï¸ Apply Missing Value Handling", key="apply_missing"):
                st.session_state.processed_data = apply_missing_values_step(st.session_state.processed_data, missing_config)
                st.session_state.show_missing_config = False
                st.success("âœ… Missing value handling applied!")
                st.rerun()
    
    # Step 2: Outlier Detection
    step2_col1, step2_col2 = st.columns([1, 3])
    with step2_col1:
        enable_outliers = st.checkbox("âœ… Apply Outlier Detection", key="enable_outliers")
    with step2_col2:
        if enable_outliers:
            st.session_state.show_outlier_config = True
    
    if enable_outliers and st.session_state.get('show_outlier_config', False):
        with st.expander("ğŸ“Š Outlier Detection Configuration", expanded=True):
            outlier_config = configure_outlier_detection_step(st.session_state.processed_data)
            if st.button("â–¶ï¸ Apply Outlier Detection", key="apply_outliers"):
                st.session_state.processed_data, outlier_info = apply_outlier_detection_step(st.session_state.processed_data, outlier_config)
                st.session_state.outlier_info = outlier_info
                st.session_state.show_outlier_config = False
                st.success("âœ… Outlier detection applied!")
                st.rerun()
    
    # Step 3: Feature Engineering
    step3_col1, step3_col2 = st.columns([1, 3])
    with step3_col1:
        enable_feature_eng = st.checkbox("âœ… Apply Feature Engineering", key="enable_feature_eng")
    with step3_col2:
        if enable_feature_eng:
            st.session_state.show_feature_eng_config = True
    
    if enable_feature_eng and st.session_state.get('show_feature_eng_config', False):
        with st.expander("ğŸ”§ Feature Engineering Configuration", expanded=True):
            feature_eng_config = configure_feature_engineering_step(st.session_state.processed_data)
            if st.button("â–¶ï¸ Apply Feature Engineering", key="apply_feature_eng"):
                st.session_state.processed_data, new_features = apply_feature_engineering_step(st.session_state.processed_data, feature_eng_config)
                st.session_state.new_features = new_features
                st.session_state.show_feature_eng_config = False
                st.success("âœ… Feature engineering applied!")
                st.rerun()
    
    # Step 4: Normalization
    step4_col1, step4_col2 = st.columns([1, 3])
    with step4_col1:
        enable_normalization = st.checkbox("âœ… Apply Normalization", key="enable_normalization")
    with step4_col2:
        if enable_normalization:
            st.session_state.show_normalization_config = True

    if enable_normalization and st.session_state.get('show_normalization_config', False):
        with st.expander("ğŸ“ Normalization Configuration", expanded=True):
            normalization_config = configure_normalization_step(st.session_state.processed_data)
            if st.button("â–¶ï¸ Apply Normalization", key="apply_normalization"):
                st.session_state.processed_data = apply_normalization_step(st.session_state.processed_data, normalization_config)
                st.session_state.show_normalization_config = False
                st.success("âœ… Normalization applied!")
                st.rerun()

    # Current Processed Data Preview with RESET functionality
    if 'processed_data' in st.session_state:
        st.markdown("---")
        st.subheader("ğŸ“‹ Current Processed Data Preview")
        
        # NEW: Add reset button
        col1, col2, col3 = st.columns([2, 1, 1])
        with col1:
            st.write("**Current processed data:**")
        with col2:
            if st.button("ğŸ”„ Reset to Original", key="reset_to_original", help="Reset data to original uploaded state"):
                if hasattr(st.session_state, 'original_raw_data') and st.session_state.original_raw_data is not None:
                    st.session_state.processed_data = st.session_state.original_raw_data.copy()
                    st.session_state.raw_data = st.session_state.original_raw_data.copy()
                    # Clear preprocessing flags
                    st.session_state.show_missing_config = False
                    st.session_state.show_outlier_config = False
                    st.session_state.show_feature_eng_config = False
                    st.session_state.show_normalization_config = False
                    st.success("âœ… Data reset to original state!")
                    st.rerun()
                else:
                    st.error("âŒ Original data not available")
        with col3:
            if st.button("ğŸ“Š Show Comparison", key="show_comparison"):
                if hasattr(st.session_state, 'original_raw_data'):
                    show_data_comparison(st.session_state.original_raw_data, st.session_state.processed_data)
        
        st.dataframe(st.session_state.processed_data.head(), use_container_width=True)

        original_shape = data.shape
        processed_shape = st.session_state.processed_data.shape
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Rows", processed_shape[0], processed_shape[0] - original_shape[0])
        with col2:
            st.metric("Columns", processed_shape[1], processed_shape[1] - original_shape[1])
        with col3:
            missing_pct = (st.session_state.processed_data.isnull().sum().sum() / (processed_shape[0] * processed_shape[1])) * 100
            st.metric("Missing %", f"{missing_pct:.1f}%")

    st.markdown("---")
    st.subheader("ğŸ”¬ Final Validation")
    
    if st.button("ğŸ” Validate Processed Data", key="validate_data"):
        validation_results = validate_processed_data(st.session_state.processed_data)
        display_validation_results(validation_results)
        
        if validation_results['is_valid']:
            st.session_state.cleaned_data = st.session_state.processed_data
            st.success("âœ… Data validation passed! Ready for feature selection.")
        else:
            st.error("âŒ Data validation failed. Please fix the issues above.")

    col1, col2 = st.columns(2)
    with col1:
        if st.button("â¬…ï¸ Back to Upload", key="back_to_upload"):
            st.session_state.training_step = 1
            st.rerun()
    
    with col2:
        if hasattr(st.session_state, 'cleaned_data') and st.session_state.cleaned_data is not None:
            if st.button("â¡ï¸ Continue to Feature Selection", key="proceed_to_feature_selection", type="primary"):
                st.session_state.training_step = 3
                st.rerun()

def show_data_comparison(original_data, processed_data):
    """NEW: Show comparison between original and processed data"""
    st.subheader("ğŸ“Š Data Comparison: Original vs Processed")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("**Original Data:**")
        st.write(f"- Shape: {original_data.shape}")
        st.write(f"- Missing values: {original_data.isnull().sum().sum()}")
        st.write(f"- Data types: {original_data.dtypes.value_counts().to_dict()}")
        
    with col2:
        st.write("**Processed Data:**")
        st.write(f"- Shape: {processed_data.shape}")
        st.write(f"- Missing values: {processed_data.isnull().sum().sum()}")
        st.write(f"- Data types: {processed_data.dtypes.value_counts().to_dict()}")
    
    # Show changes
    shape_change = (processed_data.shape[0] - original_data.shape[0], processed_data.shape[1] - original_data.shape[1])
    missing_change = processed_data.isnull().sum().sum() - original_data.isnull().sum().sum()
    
    st.write("**Changes Applied:**")
    if shape_change[0] != 0:
        st.write(f"- Rows: {shape_change[0]:+d}")
    if shape_change[1] != 0:
        st.write(f"- Columns: {shape_change[1]:+d}")
    if missing_change != 0:
        st.write(f"- Missing values: {missing_change:+d}")

def configure_missing_values_step(data):
    """Configure missing values handling"""
    st.write("**ğŸ§¹ Missing Values Configuration**")
    
    missing_info = data.isnull().sum()
    
    if missing_info.sum() > 0:
        missing_cols = missing_info[missing_info > 0]
        
        fig = px.bar(
            x=missing_cols.values, 
            y=missing_cols.index,
            orientation='h',
            title="Missing Values by Column",
            labels={'x': 'Missing Count', 'y': 'Columns'}
        )
        st.plotly_chart(fig, use_container_width=True)

        strategy = st.selectbox(
            "Choose missing values strategy:",
            [
                "Remove rows with any missing values",
                "Remove rows with missing values in selected columns only",
                "Fill missing values (Imputation)"
            ]
        )
        
        config = {"strategy": strategy}
        
        if strategy == "Remove rows with missing values in selected columns only":
            columns_with_missing = missing_cols.index.tolist()
            selected_columns = st.multiselect(
                "Select columns to check for missing values:",
                columns_with_missing,
                default=columns_with_missing
            )
            config["selected_columns"] = selected_columns
        
        elif "imputation" in strategy.lower() or "fill" in strategy.lower():
            col1, col2 = st.columns(2)
            with col1:
                numeric_method = st.selectbox(
                    "Numeric columns method:",
                    ["Mean", "Median", "Mode", "Zero"]
                )
                config["numeric_method"] = numeric_method
            
            with col2:
                categorical_method = st.selectbox(
                    "Categorical columns method:",
                    ["Mode", "A constant value"]
                )
                config["categorical_method"] = categorical_method
                
                if categorical_method == "A constant value":
                    categorical_fill_value = st.text_input("Fill value:", "Unknown")
                    config["categorical_fill_value"] = categorical_fill_value
        
        return config
    else:
        st.success("âœ… No missing values found in the dataset!")
        return {"strategy": "none"}

def apply_missing_values_step(data, config):
    """Apply missing values handling"""
    if config["strategy"] == "none":
        return data
        
    return handle_missing_values(
        data, 
        config["strategy"], 
        config.get("selected_columns"),
        config.get("numeric_method", "Mean"),
        config.get("categorical_method", "Mode"),
        config.get("categorical_fill_value", "Unknown")
    )

def configure_outlier_detection_step(data):
    """Configure outlier detection"""
    st.write("**ğŸ“Š Outlier Detection Configuration**")
    
    numeric_columns = data.select_dtypes(include=[np.number]).columns.tolist()
    
    if len(numeric_columns) == 0:
        st.info("â„¹ï¸ No numeric columns found for outlier detection.")
        return {"method": "none"}
    
    detection_method = st.selectbox(
        "Choose outlier detection method:",
        ["IQR", "Z-Score", "Modified Z-Score"]
    )

    treatment_action = st.selectbox(
        "Choose treatment action:",
        ["mark", "remove", "clip"]
    )
    
    config = {
        "method": detection_method,
        "action": treatment_action
    }
    
    if st.button("ğŸ” Preview Outlier Detection", key="preview_outliers"):
        with st.spinner("Detecting outliers..."):
            try:
                _, outlier_info = detect_and_handle_outliers(data, method=detection_method, action="mark")
                
                if outlier_info:
                    st.subheader("ğŸ“ˆ Outlier Detection Results")
                    
                    outlier_summary = []
                    for col, info in outlier_info.items():
                        if info['count'] > 0:
                            outlier_summary.append({
                                'Column': col,
                                'Outliers': info['count'],
                                'Percentage': f"{info['percentage']:.2f}%"
                            })
                    
                    if outlier_summary:
                        st.dataframe(pd.DataFrame(outlier_summary), use_container_width=True)
                    else:
                        st.success("âœ… No outliers detected!")
                
            except Exception as e:
                st.error(f"Error detecting outliers: {str(e)}")
    
    return config

def apply_outlier_detection_step(data, config):
    """Apply outlier detection"""
    if config["method"] == "none":
        return data, {}
    
    return detect_and_handle_outliers(data, method=config["method"], action=config["action"])

def configure_feature_engineering_step(data):
    """FIXED: Configure feature engineering with proper encoding options"""
    st.write("**ğŸ”§ Feature Engineering Configuration**")
    
    config = {}
    
    categorical_columns = data.select_dtypes(include=['object', 'category']).columns.tolist()
    
    if categorical_columns:
        st.write("**ğŸ·ï¸ Categorical Encoding**")
        
        encoding_method = st.selectbox(
            "Choose encoding method:",
            ["Auto", "One-Hot Encoding", "Label Encoding"]
        )
        config["encoding_method"] = encoding_method
        
        if encoding_method == "Auto":
            cardinality_threshold = st.slider(
                "Cardinality threshold for one-hot encoding:",
                2, 20, 10,
                help="Columns with fewer unique values will use one-hot encoding, others will use label encoding"
            )
            config["cardinality_threshold"] = cardinality_threshold
    
    # Interaction Features
    st.write("**â• Interaction Features**")
    
    create_interactions = st.checkbox(
        "Create polynomial and interaction features",
        help="Create polynomial and interaction features for numeric columns"
    )
    config["create_interactions"] = create_interactions
    
    if create_interactions:
        interaction_degree = st.slider("Polynomial degree:", 2, 3, 2)
        config["interaction_degree"] = interaction_degree
        
        numeric_columns = data.select_dtypes(include=[np.number]).columns.tolist()
        if numeric_columns:
            selected_interaction_cols = st.multiselect(
                "Select columns for interaction features:",
                numeric_columns,
                default=numeric_columns[:3] if len(numeric_columns) > 3 else numeric_columns
            )
            config["selected_interaction_cols"] = selected_interaction_cols
    
    return config

def apply_feature_engineering_step(data, config):
    """FIXED: Apply feature engineering with proper encoding"""
    return advanced_feature_engineering(
        data,
        encoding_method=config.get("encoding_method", "Auto"),
        cardinality_threshold=config.get("cardinality_threshold", 10),
        create_interactions=config.get("create_interactions", False),
        interaction_degree=config.get("interaction_degree", 2),
        selected_interaction_cols=config.get("selected_interaction_cols", [])
    )

def configure_normalization_step(data):
    """Configure normalization"""
    st.write("**ğŸ“ Normalization Configuration**")
    
    numeric_columns = data.select_dtypes(include=[np.number]).columns.tolist()
    
    if len(numeric_columns) == 0:
        st.info("â„¹ï¸ No numeric columns found for normalization.")
        return {"method": "none"}
    
    normalization_method = st.selectbox(
        "Choose normalization method:",
        ["Min-Max Scaling", "Standard Scaling", "Robust Scaling"]
    )
    
    selected_numeric_cols = st.multiselect(
        "Select columns to normalize:",
        numeric_columns,
        default=numeric_columns,
        help="Choose which numeric columns to normalize"
    )
    
    config = {
        "method": normalization_method,
        "selected_columns": selected_numeric_cols
    }
    
    return config

def apply_normalization_step(data, config):
    """Apply normalization"""
    if config["method"] == "none":
        return data
    
    return normalize_data(data, method=config["method"], selected_columns=config.get("selected_columns"))

def validate_processed_data(data):
    """Validate processed data"""
    validation_results = {
        "is_valid": True,
        "issues": [],
        "warnings": [],
        "info": []
    }
    
    # Check for missing values
    missing_count = data.isnull().sum().sum()
    if missing_count > 0:
        validation_results["warnings"].append(f"Found {missing_count} missing values")
    else:
        validation_results["info"].append("No missing values found")
    
    # Check for non-numeric columns
    non_numeric_cols = []
    for col in data.columns:
        if not pd.api.types.is_numeric_dtype(data[col]):
            non_numeric_cols.append(col)
    
    if non_numeric_cols:
        validation_results["issues"].append(f"Non-numeric columns found: {non_numeric_cols}")
        validation_results["is_valid"] = False
    else:
        validation_results["info"].append("All columns are numeric")
    
    # Check for infinite values
    inf_count = np.isinf(data.select_dtypes(include=[np.number])).sum().sum()
    if inf_count > 0:
        validation_results["warnings"].append(f"Found {inf_count} infinite values")
    else:
        validation_results["info"].append("No infinite values found")
    
    # Check dataset size
    if data.shape[0] == 0:
        validation_results["issues"].append("Dataset is empty (0 rows)")
        validation_results["is_valid"] = False
    elif data.shape[1] == 0:
        validation_results["issues"].append("Dataset has no columns")
        validation_results["is_valid"] = False
    else:
        validation_results["info"].append(f"Dataset shape: {data.shape}")
    
    return validation_results

def display_validation_results(validation_results):
    """Display validation results"""
    if validation_results["is_valid"]:
        st.success("âœ… Data validation passed!")
    else:
        st.error("âŒ Data validation failed!")
    
    if validation_results["issues"]:
        st.error("**Issues that need to be fixed:**")
        for issue in validation_results["issues"]:
            st.error(f"â€¢ {issue}")
    
    if validation_results["warnings"]:
        st.warning("**Warnings:**")
        for warning in validation_results["warnings"]:
            st.warning(f"â€¢ {warning}")
    
    if validation_results["info"]:
        st.info("**Information:**")
        for info in validation_results["info"]:
            st.info(f"â€¢ {info}")

def enhanced_feature_selection_step():
    """Enhanced feature selection with automated methods"""
    st.header("3. ğŸ¯ Advanced Feature Selection & Labeling")
    
    if not hasattr(st.session_state, 'cleaned_data') or st.session_state.cleaned_data is None:
        st.warning("âš ï¸ No processed data available. Please complete preprocessing first.")
        if st.button("â¬…ï¸ Back to Preprocessing", key="back_to_preprocessing_from_feature"):
            st.session_state.training_step = 2
            st.rerun()
        return
    
    feature_selection_page()
    
    col1, col2 = st.columns(2)
    with col1:
        if st.button("â¬…ï¸ Back to Preprocessing", key="back_to_preprocessing"):
            st.session_state.training_step = 2
            st.rerun()
    
    with col2:
        if hasattr(st.session_state, 'labeled_data') and st.session_state.labeled_data is not None:
            if st.button("â¡ï¸ Continue to Model Training", key="proceed_to_model_training", type="primary"):
                st.session_state.training_step = 4
                st.rerun()

def enhanced_model_training_step():
    """Enhanced model training with advanced options"""
    st.header("4. ğŸ¤– Advanced Model Training & Optimization")
    
    if not hasattr(st.session_state, 'labeled_data') or st.session_state.labeled_data is None:
        st.warning("âš ï¸ No labeled data available. Please complete feature selection first.")
        if st.button("â¬…ï¸ Back to Feature Selection", key="back_to_feature_selection"):
            st.session_state.training_step = 3
            st.rerun()
        return
    
    data = st.session_state.labeled_data
    target_column = st.session_state.target_column
    feature_columns = st.session_state.feature_columns
    
    # Dataset Summary
    st.subheader("ğŸ“Š Dataset Summary")
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("Total Samples", len(data))
    with col2:
        st.metric("Features", len(feature_columns))
    with col3:
        st.metric("Target", target_column)
    with col4:
        missing_count = data.isnull().sum().sum()
        st.metric("Missing Values", missing_count)
    
    # Detect task type
    is_classification = data[target_column].dtype == 'object' or len(data[target_column].unique()) < 20
    task_type = "Classification" if is_classification else "Regression"
    
    st.info(f"ğŸ¯ **Detected Task Type:** {task_type}")
    
    # Model Training Tabs
    tab1, tab2, tab3, tab4 = st.tabs([
        "ğŸ¯ Model Selection", 
        "ğŸ”§ Hyperparameter Tuning", 
        "ğŸ“Š Model Comparison", 
        "âš™ï¸ Training Configuration"
    ])
    
    with tab1:
        model_selection_section(is_classification)
    
    with tab2:
        hyperparameter_tuning_section(data, target_column, feature_columns, is_classification)
    
    with tab3:
        model_comparison_section(data, target_column, feature_columns, is_classification)
    
    with tab4:
        training_configuration_section()
    
    # Execute Training
    st.markdown("---")
    st.subheader("ğŸš€ Execute Training")
    
    if st.button("â–¶ï¸ Start Training Process", type="primary", use_container_width=True):
        execute_training_process(data, target_column, feature_columns, is_classification)
    
    # Navigation
    col1, col2 = st.columns(2)
    with col1:
        if st.button("â¬…ï¸ Back to Feature Selection", key="back_to_feature_selection_from_training"):
            st.session_state.training_step = 3
            st.rerun()
    
    with col2:
        if hasattr(st.session_state, 'trained_model') and st.session_state.trained_model is not None:
            if st.button("â¡ï¸ Continue to Evaluation", key="proceed_to_evaluation", type="secondary"):
                st.session_state.training_step = 5
                st.rerun()

def model_selection_section(is_classification):
    """Model selection with intelligent suggestions"""
    st.subheader("ğŸ¯ Intelligent Model Selection")
    
    available_models = get_available_models(True)
    
    # Filter models based on task type
    if is_classification:
        model_options = [k for k in available_models.keys() if 'Classifier' in k or k in ['Logistic Regression', 'Gaussian Naive Bayes']]
    else:
        model_options = [k for k in available_models.keys() if 'Regressor' in k or k == 'Linear Regression']
    
    # Model selection
    col1, col2 = st.columns([2, 1])
    
    with col1:
        selected_model = st.selectbox(
            "Choose model algorithm:",
            model_options,
            help="Select the machine learning algorithm to train"
        )
        st.session_state.selected_model = selected_model
    
    with col2:
        st.markdown("### ğŸ’¡ Model Recommendations")
        if is_classification:
            st.info("""
            **For Classification:**
            - **Random Forest**: Good baseline, handles mixed data
            - **Gradient Boosting**: High performance, robust
            - **SVM**: Good for small-medium datasets
            - **Logistic Regression**: Fast, interpretable
            """)
        else:
            st.info("""
            **For Regression:**
            - **Random Forest**: Good baseline, robust
            - **Gradient Boosting**: High performance
            - **Linear Regression**: Fast, interpretable
            - **SVR**: Good for non-linear patterns
            """)
    
    # Model parameters
    if selected_model in available_models:
        st.subheader("âš™ï¸ Model Parameters")
        params_config = available_models[selected_model]["params"]
        model_params = {}
        
        # Create parameter inputs
        param_cols = st.columns(2)
        param_idx = 0
        
        for param_name, param_config in params_config.items():
            with param_cols[param_idx % 2]:
                if param_config["type"] == "integer":
                    model_params[param_name] = st.slider(
                        f"{param_name}:",
                        min_value=param_config["min"],
                        max_value=param_config["max"],
                        value=param_config["default"],
                        step=param_config["step"],
                        help=param_config["help"]
                    )
                elif param_config["type"] == "numeric":
                    model_params[param_name] = st.slider(
                        f"{param_name}:",
                        min_value=float(param_config["min"]),
                        max_value=float(param_config["max"]),
                        value=float(param_config["default"]),
                        step=float(param_config["step"]),
                        help=param_config["help"]
                    )
                elif param_config["type"] == "select":
                    model_params[param_name] = st.selectbox(
                        f"{param_name}:",
                        param_config["options"],
                        index=param_config["options"].index(param_config["default"]),
                        help=param_config["help"]
                    )
                elif param_config["type"] == "boolean":
                    model_params[param_name] = st.checkbox(
                        f"{param_name}:",
                        value=param_config["default"],
                        help=param_config["help"]
                    )
            
            param_idx += 1
        
        st.session_state.model_params = model_params

def hyperparameter_tuning_section(data, target_column, feature_columns, is_classification):
    """Advanced hyperparameter tuning options"""
    st.subheader("ğŸ”§ Automated Hyperparameter Tuning")
    
    enable_tuning = st.checkbox(
        "Enable hyperparameter tuning",
        value=False,
        help="Automatically find optimal parameters"
    )
    
    if enable_tuning:
        col1, col2, col3 = st.columns(3)
        
        with col1:
            search_method = st.selectbox(
                "Search method:",
                ["Grid Search", "Random Search"],
                help="Method for hyperparameter optimization"
            )
        
        with col2:
            cv_folds = st.slider(
                "Cross-validation folds:",
                3, 10, 5,
                help="Number of folds for cross-validation"
            )
        
        with col3:
            if search_method == "Random Search":
                max_iter = st.slider(
                    "Max iterations:",
                    10, 100, 50,
                    help="Maximum number of parameter combinations to try"
                )
            else:
                max_iter = None
        
        # Store tuning configuration
        st.session_state.enable_tuning = enable_tuning
        st.session_state.search_method = search_method
        st.session_state.cv_folds = cv_folds
        st.session_state.max_iter = max_iter
        
        # Preview hyperparameter space
        if st.button("ğŸ” Preview Hyperparameter Space"):
            if hasattr(st.session_state, 'selected_model'):
                selected_model = st.session_state.selected_model
                available_models = get_available_models(True)
                
                if selected_model in available_models:
                    param_grid = available_models[selected_model].get("param_grid", {})
                    
                    if param_grid:
                        st.write("**Parameter search space:**")
                        for param, values in param_grid.items():
                            st.write(f"- **{param}**: {values}")
                        
                        # Calculate total combinations
                        total_combinations = 1
                        for values in param_grid.values():
                            total_combinations *= len(values)
                        
                        if search_method == "Grid Search":
                            st.info(f"ğŸ“Š Total combinations: {total_combinations}")
                        else:
                            actual_iter = min(max_iter, total_combinations)
                            st.info(f"ğŸ“Š Will test {actual_iter} random combinations")
                    else:
                        st.warning("âš ï¸ No parameter grid defined for this model")

def model_comparison_section(data, target_column, feature_columns, is_classification):
    """Compare multiple models automatically"""
    st.subheader("ğŸ“Š Automated Model Comparison")
    
    enable_comparison = st.checkbox(
        "Enable model comparison",
        value=False,
        help="Compare multiple algorithms automatically"
    )
    
    if enable_comparison:
        available_models = get_available_models(True)
        
        # Filter models based on task type
        if is_classification:
            all_models = [k for k in available_models.keys() if 'Classifier' in k or k in ['Logistic Regression', 'Gaussian Naive Bayes']]
        else:
            all_models = [k for k in available_models.keys() if 'Regressor' in k or k == 'Linear Regression']
        
        # Model selection for comparison
        models_to_compare = st.multiselect(
            "Select models to compare:",
            all_models,
            default=all_models[:3] if len(all_models) >= 3 else all_models,
            help="Choose models for automatic comparison"
        )
        
        comparison_cv_folds = st.slider(
            "Cross-validation folds for comparison:",
            3, 10, 5,
            help="Number of folds for model comparison"
        )
        
        # Store comparison configuration
        st.session_state.enable_comparison = enable_comparison
        st.session_state.models_to_compare = models_to_compare
        st.session_state.comparison_cv_folds = comparison_cv_folds
        
        # Preview model comparison
        if st.button("ğŸ” Preview Model Comparison"):
            if models_to_compare:
                st.write(f"**Will compare {len(models_to_compare)} models:**")
                for model in models_to_compare:
                    st.write(f"- {model}")
                
                estimated_time = len(models_to_compare) * comparison_cv_folds * 2  # rough estimate
                st.info(f"â±ï¸ Estimated comparison time: ~{estimated_time} seconds")

def training_configuration_section():
    """Advanced training configuration options"""
    st.subheader("âš™ï¸ Training Configuration")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        test_size = st.slider(
            "Test set size:",
            0.1, 0.4, 0.2, 0.05,
            help="Fraction of data to use for testing"
        )
    
    with col2:
        random_state = st.number_input(
            "Random seed:",
            0, 1000, 42,
            help="Random seed for reproducibility"
        )
    
    with col3:
        use_stratify = st.checkbox(
            "Stratified sampling",
            value=True,
            help="Use stratified sampling for train/test split"
        )
    
    # Cross-validation settings
    st.write("**ğŸ” Cross-Validation Settings**")
    
    col1, col2 = st.columns(2)
    
    with col1:
        enable_cv = st.checkbox(
            "Enable cross-validation",
            value=True,
            help="Perform cross-validation during training"
        )
    
    with col2:
        if enable_cv:
            cv_folds_training = st.slider(
                "CV folds:",
                3, 10, 5,
                help="Number of cross-validation folds"
            )
        else:
            cv_folds_training = 5
    
    # Store training configuration
    st.session_state.test_size = test_size
    st.session_state.random_state = random_state
    st.session_state.use_stratify = use_stratify
    st.session_state.enable_cv = enable_cv
    st.session_state.cv_folds_training = cv_folds_training

def execute_training_process(data, target_column, feature_columns, is_classification):
    """Execute the complete training process with proper LabelEncoder handling"""
    with st.spinner("ğŸš€ Executing training process..."):
        try:
            features = data[feature_columns]
            target = data[target_column]
            
            test_size = getattr(st.session_state, 'test_size', 0.2)
            random_state = getattr(st.session_state, 'random_state', 42)
            use_stratify = getattr(st.session_state, 'use_stratify', True)
            
            results = {}
            
            # Model comparison
            if getattr(st.session_state, 'enable_comparison', False):
                st.write("ğŸ“Š Running model comparison...")
                models_to_compare = getattr(st.session_state, 'models_to_compare', [])
                comparison_cv_folds = getattr(st.session_state, 'comparison_cv_folds', 5)
                
                if models_to_compare:
                    comparison_results = compare_models(
                        features, target, models_to_compare, comparison_cv_folds
                    )
                    results['model_comparison'] = comparison_results
                    
                    recommendations = get_model_recommendations(comparison_results, is_classification)
                    results['recommendations'] = recommendations
                    
                    st.subheader("ğŸ“ˆ Model Comparison Results")
                    
                    comparison_df = pd.DataFrame({
                        model: {
                            'Mean Score': result.get('mean_score', 0),
                            'Std Score': result.get('std_score', 0),
                            'Min Score': result.get('min_score', 0),
                            'Max Score': result.get('max_score', 0)
                        }
                        for model, result in comparison_results.items()
                        if 'error' not in result
                    }).T
                    
                    st.dataframe(comparison_df, use_container_width=True)
                    
                    if len(comparison_df) > 0:
                        fig = px.bar(
                            x=comparison_df.index,
                            y=comparison_df['Mean Score'],
                            error_y=comparison_df['Std Score'],
                            title="Model Comparison - Mean Cross-Validation Score"
                        )
                        st.plotly_chart(fig, use_container_width=True)
                    
                    if 'error' not in recommendations:
                        st.success(f"ğŸ† **Best Model:** {recommendations['best_model']}")
                        st.info(f"ğŸ“Š **Best Score:** {recommendations['best_score']:.4f}")
            
            # Hyperparameter tuning
            best_params = None
            if getattr(st.session_state, 'enable_tuning', False):
                st.write("ğŸ”§ Running hyperparameter tuning...")
                
                selected_model = getattr(st.session_state, 'selected_model', None)
                search_method = getattr(st.session_state, 'search_method', 'Grid Search')
                cv_folds = getattr(st.session_state, 'cv_folds', 5)
                max_iter = getattr(st.session_state, 'max_iter', 50)
                
                if selected_model:
                    tuning_results = hyperparameter_tuning(
                        features, target, selected_model,
                        search_method.lower().replace(' ', '_'),
                        max_iter, cv_folds
                    )
                    
                    if 'error' not in tuning_results:
                        best_params = tuning_results['best_params']
                        results['hyperparameter_tuning'] = tuning_results
                        
                        st.subheader("ğŸ¯ Hyperparameter Tuning Results")
                        st.success(f"**Best Score:** {tuning_results['best_score']:.4f}")
                        st.write("**Best Parameters:**")
                        for param, value in best_params.items():
                            st.write(f"- **{param}**: {value}")
                    else:
                        st.error(f"Error in hyperparameter tuning: {tuning_results['error']}")
            
            # Train final model
            st.write("ğŸ—ï¸ Training final model...")

            if best_params:
                final_params = best_params
            else:
                final_params = getattr(st.session_state, 'model_params', {})
            
            selected_model = getattr(st.session_state, 'selected_model', 'Random Forest Classifier')
            
            trained_model, X_train, X_test, y_train, y_test, label_encoder = train_model(
                features, target, selected_model, final_params,
                test_size, random_state, use_stratify
            )
            
            # Cross-validation
            cv_results = None
            if getattr(st.session_state, 'enable_cv', True):
                st.write("ğŸ” Performing cross-validation...")
                cv_folds_training = getattr(st.session_state, 'cv_folds_training', 5)
                
                cv_results = enhanced_cross_validation(
                    features, target, selected_model, final_params, cv_folds_training
                )
                results['cv_results'] = cv_results
            
            # Store training information
            training_info = {
                'model': trained_model,
                'model_name': selected_model,
                'feature_columns': feature_columns,
                'target_column': target_column,
                'X_train': X_train,
                'X_test': X_test,
                'y_train': y_train,
                'y_test': y_test,
                'params': final_params,
                'test_size': test_size,
                'random_state': random_state,
                'label_encoder': label_encoder,  
                'is_classification': is_classification
            }
            
            st.session_state.label_encoder = label_encoder
            st.session_state.trained_model = training_info
            st.session_state.training_results = results
            
            st.success("âœ… Training completed successfully!")
            
            display_training_summary(training_info, results, cv_results)
            
        except Exception as e:
            st.error(f"âŒ Error during training: {str(e)}")
            st.error("ğŸ’¡ Please check your configuration and try again.")

def display_training_summary(training_info, results, cv_results):
    """Display comprehensive training summary"""
    st.subheader("ğŸ“‹ Training Summary")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("Model", training_info['model_name'])
    with col2:
        st.metric("Training Samples", len(training_info['X_train']))
    with col3:
        st.metric("Test Samples", len(training_info['X_test']))
    with col4:
        st.metric("Features", len(training_info['feature_columns']))
    
    if training_info.get('label_encoder') is not None:
        st.success("ğŸ·ï¸ **Label Encoder Active**: Predictions will show original string labels")
        
        try:
            label_encoder = training_info['label_encoder']
            original_classes = label_encoder.classes_
            
            st.write("**Target Label Mapping:**")
            mapping_data = []
            for i, cls in enumerate(original_classes):
                mapping_data.append({"Original Label": cls, "Numeric Code": i})
            
            mapping_df = pd.DataFrame(mapping_data)
            st.dataframe(mapping_df, use_container_width=True)
            
        except Exception as e:
            st.warning(f"âš ï¸ Could not display label mapping: {e}")
    
    if cv_results:
        st.subheader("ğŸ” Cross-Validation Results")
        
        cv_summary = []
        for metric, result in cv_results.items():
            if result['mean'] is not None:
                cv_summary.append({
                    'Metric': metric,
                    'Mean': f"{result['mean']:.4f}",
                    'Std Dev': f"{result['std']:.4f}",
                    'Min': f"{result['min']:.4f}",
                    'Max': f"{result['max']:.4f}"
                })
        
        if cv_summary:
            st.dataframe(pd.DataFrame(cv_summary), use_container_width=True)
    
    st.subheader("âš™ï¸ Final Model Parameters")
    params_df = pd.DataFrame(list(training_info['params'].items()), columns=['Parameter', 'Value'])
    st.dataframe(params_df, use_container_width=True)

def enhanced_model_evaluation_step():
    """Enhanced model evaluation with comprehensive analysis"""
    st.header("5. ğŸ“ˆ Advanced Model Evaluation & Analysis")
    
    if not hasattr(st.session_state, 'trained_model') or st.session_state.trained_model is None:
        st.warning("âš ï¸ No trained model available. Please complete model training first.")
        if st.button("â¬…ï¸ Back to Training", key="back_to_training"):
            st.session_state.training_step = 4
            st.rerun()
        return
    
    training_info = st.session_state.trained_model
    model = training_info['model']
    X_test = training_info['X_test']
    y_test = training_info['y_test']
    label_encoder = training_info.get('label_encoder')
    
    # Evaluate model
    with st.spinner("ğŸ” Evaluating model performance..."):
        evaluation_results = evaluate_model(model, X_test, y_test, True)
        
        if label_encoder is not None and 'accuracy' in evaluation_results:
            try:
                y_test_original = label_encoder.inverse_transform(y_test)
                evaluation_results['y_test_original'] = y_test_original
                evaluation_results['label_encoder'] = label_encoder
            except:
                pass
        
        # Add training results if available
        if hasattr(st.session_state, 'training_results'):
            training_results = st.session_state.training_results
            if 'cv_results' in training_results:
                evaluation_results['cv_results'] = training_results['cv_results']
        
        st.session_state.model_evaluation = evaluation_results
    
    # Display evaluation results in tabs
    tab1, tab2, tab3, tab4, tab5 = st.tabs([
        "ğŸ“Š Performance Metrics",
        "ğŸ“ˆ Visualizations", 
        "ğŸ” Feature Analysis",
        "ğŸ“‹ Detailed Reports",
        "ğŸ’¾ Export & Deploy"
    ])
    
    with tab1:
        display_performance_metrics(evaluation_results)
    
    with tab2:
        display_evaluation_visualizations(evaluation_results)
    
    with tab3:
        display_feature_analysis(evaluation_results, training_info)
    
    with tab4:
        display_detailed_reports(evaluation_results, training_info)
    
    with tab5:
        export_and_deployment_section(training_info, evaluation_results)
    
    # Navigation
    col1, col2 = st.columns(2)
    with col1:
        if st.button("â¬…ï¸ Back to Training", key="back_to_training_from_eval"):
            st.session_state.training_step = 4
            st.rerun()
    
    with col2:
        if st.button("ğŸ  Return to Home", key="return_to_home"):
            st.session_state.current_page = 'home'
            st.rerun()

def display_performance_metrics(evaluation_results):
    """Display comprehensive performance metrics"""
    st.subheader("ğŸ“Š Performance Metrics Overview")
    
    # Determine if classification or regression
    is_classification = 'accuracy' in evaluation_results
    
    if is_classification:
        # Classification metrics
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            accuracy = evaluation_results.get('accuracy', 0)
            st.metric("Accuracy", f"{accuracy:.4f}", f"{accuracy*100:.2f}%")
        
        with col2:
            precision = evaluation_results.get('precision', 0)
            st.metric("Precision", f"{precision:.4f}", f"{precision*100:.2f}%")
        
        with col3:
            recall = evaluation_results.get('recall', 0)
            st.metric("Recall", f"{recall:.4f}", f"{recall*100:.2f}%")
        
        with col4:
            f1 = evaluation_results.get('f1', 0)
            st.metric("F1-Score", f"{f1:.4f}", f"{f1*100:.2f}%")
        
        # Additional classification metrics
        if 'roc_auc' in evaluation_results:
            col1, col2 = st.columns(2)
            with col1:
                roc_auc = evaluation_results['roc_auc']
                st.metric("ROC AUC", f"{roc_auc:.4f}")
            
            if 'average_precision' in evaluation_results:
                with col2:
                    avg_precision = evaluation_results['average_precision']
                    st.metric("Average Precision", f"{avg_precision:.4f}")
    
    else:
        # Regression metrics
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            r2 = evaluation_results.get('r2', 0)
            st.metric("RÂ² Score", f"{r2:.4f}", f"{r2*100:.2f}%")
        
        with col2:
            rmse = evaluation_results.get('rmse', 0)
            st.metric("RMSE", f"{rmse:.4f}")
        
        with col3:
            mae = evaluation_results.get('mae', 0)
            st.metric("MAE", f"{mae:.4f}")
        
        with col4:
            mape = evaluation_results.get('mape', 0)
            if not np.isnan(mape):
                st.metric("MAPE", f"{mape:.2f}%")
            else:
                st.metric("MAPE", "N/A")
    
    # Cross-validation results
    if 'cv_results' in evaluation_results:
        st.subheader("ğŸŸ¢ Cross-Validation Results")
        cv_results = evaluation_results['cv_results']
        
        cv_data = []
        for metric, result in cv_results.items():
            if result.get('mean') is not None:
                cv_data.append({
                    'Metric': metric.replace('_', ' ').title(),
                    'Mean': f"{result['mean']:.4f}",
                    'Std Dev': f"{result['std']:.4f}",
                    'Range': f"{result['min']:.4f} - {result['max']:.4f}"
                })
        
        if cv_data:
            st.dataframe(pd.DataFrame(cv_data), use_container_width=True)

def display_evaluation_visualizations(evaluation_results):
    """Display evaluation visualizations"""
    st.subheader("ğŸ“ˆ Performance Visualizations")
    
    # Determine if classification or regression
    is_classification = 'accuracy' in evaluation_results
    
    if is_classification:
        # Classification visualizations
        
        # Confusion Matrix
        if 'interactive_confusion_matrix' in evaluation_results:
            st.subheader("ğŸ”¥ Interactive Confusion Matrix")
            st.plotly_chart(evaluation_results['interactive_confusion_matrix'], use_container_width=True)
        
        # ROC Curve
        if 'roc_curve' in evaluation_results:
            st.subheader("ğŸ“ˆ ROC Curve")
            st.plotly_chart(evaluation_results['roc_curve'], use_container_width=True)
        
        # Precision-Recall Curve
        if 'pr_curve' in evaluation_results:
            st.subheader("ğŸ“Š Precision-Recall Curve")
            st.plotly_chart(evaluation_results['pr_curve'], use_container_width=True)
        
        # Class Distribution
        if 'class_distribution_plot' in evaluation_results:
            st.subheader("ğŸ“Š Class Distribution Analysis")
            st.plotly_chart(evaluation_results['class_distribution_plot'], use_container_width=True)
    
    else:
        # Regression visualizations
        
        # Actual vs Predicted
        if 'actual_vs_predicted_plot' in evaluation_results:
            st.subheader("ğŸ“ˆ Actual vs Predicted Values")
            st.plotly_chart(evaluation_results['actual_vs_predicted_plot'], use_container_width=True)
        
        # Residual Analysis
        if 'residual_analysis' in evaluation_results:
            st.subheader("ğŸ” Residual Analysis")
            st.plotly_chart(evaluation_results['residual_analysis'], use_container_width=True)
        
        # Prediction Intervals
        if 'prediction_intervals_plot' in evaluation_results:
            st.subheader("ğŸ“Š Prediction Intervals")
            st.plotly_chart(evaluation_results['prediction_intervals_plot'], use_container_width=True)
            
            coverage = evaluation_results.get('prediction_interval_coverage', 0)
            st.info(f"ğŸ“Š **Prediction Interval Coverage:** {coverage:.1%}")

def display_feature_analysis(evaluation_results, training_info):
    """Display feature importance and analysis"""
    st.subheader("ğŸ” Feature Importance Analysis")
    
    if 'feature_importance_plot_interactive' in evaluation_results:
        st.plotly_chart(evaluation_results['feature_importance_plot_interactive'], use_container_width=True)
        
        # Feature importance statistics
        if 'feature_importance_stats' in evaluation_results:
            stats = evaluation_results['feature_importance_stats']
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.write("**ğŸ† Top 5 Most Important Features:**")
                for i, (feature, importance) in enumerate(zip(stats['top_features'][:5], stats['top_importances'][:5])):
                    st.write(f"{i+1}. **{feature}**: {importance:.4f}")
            
            with col2:
                features_80_pct = stats.get('features_for_80_percent', 0)
                total_features = len(training_info['feature_columns'])
                
                st.metric("Features for 80% Importance", features_80_pct)
                st.metric("Total Features", total_features)
                
                if features_80_pct > 0:
                    efficiency = (features_80_pct / total_features) * 100
                    st.metric("Feature Efficiency", f"{efficiency:.1f}%")
    
    else:
        st.info("â„¹ï¸ Feature importance not available for this model type.")

def display_detailed_reports(evaluation_results, training_info):
    """Display detailed classification/regression reports"""
    st.subheader("ğŸ“‹ Detailed Performance Reports")
    
    # Classification report
    if 'classification_report' in evaluation_results:
        st.subheader("ğŸ“Š Classification Report")
        report_df = evaluation_results['classification_report']
        
        # Style the report
        styled_report = report_df.style.format("{:.4f}").background_gradient(
            subset=['precision', 'recall', 'f1-score'], cmap='RdYlGn'
        )
        st.dataframe(styled_report, use_container_width=True)
    
    # Regression residual statistics
    if 'residual_stats' in evaluation_results:
        st.subheader("ğŸ“Š Residual Statistics")
        residual_stats = evaluation_results['residual_stats']
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("Mean Residual", f"{residual_stats['mean']:.6f}")
        with col2:
            st.metric("Std Residual", f"{residual_stats['std']:.4f}")
        with col3:
            st.metric("Skewness", f"{residual_stats['skewness']:.4f}")
        with col4:
            st.metric("Kurtosis", f"{residual_stats['kurtosis']:.4f}")
        
        # Residual analysis interpretation
        if abs(residual_stats['mean']) < 0.01:
            st.success("âœ… Residuals are well-centered around zero")
        else:
            st.warning("âš ï¸ Residuals show some bias")
        
        if abs(residual_stats['skewness']) < 0.5:
            st.success("âœ… Residuals are approximately symmetric")
        else:
            st.warning("âš ï¸ Residuals show skewness")

def export_and_deployment_section(training_info, evaluation_results):
    """Export model and generate deployment materials"""
    st.subheader("ğŸ’¾ Export & Deployment")
    
    # Model export and report generation
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("**ğŸ“¦ Model Export**")
        
        export_format = st.selectbox(
            "Export format:",
            ["joblib", "pickle"],
            help="Choose format for saving the model"
        )
        
        if 'model_filename' not in st.session_state:
            default_name = f"{training_info['model_name'].lower().replace(' ', '_')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            st.session_state.model_filename = default_name
        
        model_name = st.text_input(
            "Model filename:",
            value=st.session_state.model_filename,
            help="Filename for the saved model (without extension)",
            key="model_filename_input"
        )
        
        if model_name != st.session_state.model_filename:
            st.session_state.model_filename = model_name
        
        # Download model button
        if st.button("ğŸ’¾ Download Model", type="primary", use_container_width=True):
            try:
                with st.spinner("Preparing model for download..."):
                    label_encoder = training_info.get('label_encoder')
                    
                    model_data = save_model_for_download(
                        training_info['model'], 
                        label_encoder, 
                        st.session_state.model_filename, 
                        export_format
                    )
                    
                    filename = f"{st.session_state.model_filename}.{export_format}"
                    
                    st.download_button(
                        label="â¬‡ï¸ Download Now",
                        data=model_data,
                        file_name=filename,
                        mime="application/octet-stream",
                        use_container_width=True,
                        key="download_model_button"
                    )
                    
                    st.success(f"âœ… Model ready for download!")
                    st.info(f"ğŸ“ **Filename:** {filename}")
                    st.info(f"ğŸ“Š **Size:** {len(model_data) / 1024:.1f} KB")
                
            except Exception as e:
                st.error(f"âŒ Error preparing model for download: {str(e)}")
        
        st.markdown("---")
        st.write("**â„¹ï¸ Model Package Contents:**")
        st.write("â€¢ Trained model object")
        st.write("â€¢ Label encoder (for classification)")
        st.write("â€¢ Timestamp and metadata")
        st.write("â€¢ Model parameters")
    
    with col2:
        st.write("**ğŸ“Š Generate Reports**")
        
        if st.button("ğŸ“„ Generate HTML Report", type="secondary"):
            try:
                with st.spinner("Generating comprehensive report..."):
                    model_info = {
                        "model_object": training_info['model'],
                        "model_name": training_info['model_name'],
                        "X_train": training_info['X_train'],
                        "X_test": training_info['X_test'],
                        "y_train": training_info['y_train'],
                        "y_test": training_info['y_test'],
                        "params": training_info['params'],
                        "is_supervised": True
                    }
                    
                    model_path = f"{st.session_state.model_filename}.{export_format}"
                    
                    html_report = generate_report(model_info, evaluation_results, model_path)
                    
                    report_filename = f"ml_report_{st.session_state.model_filename}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html"
                    
                    st.download_button(
                        label="â¬‡ï¸ Download HTML Report",
                        data=html_report,
                        file_name=report_filename,
                        mime="text/html",
                        use_container_width=True
                    )
                    
                    st.success("âœ… HTML report generated successfully!")
                    st.info(f"ğŸ“ **Report:** {report_filename}")
                    st.info(f"ğŸ“Š **Size:** {len(html_report.encode()) / 1024:.1f} KB")
                
            except Exception as e:
                st.error(f"âŒ Error generating report: {str(e)}")
        
        if st.button("ğŸ“Š Export Model Summary (JSON)"):
            try:
                with st.spinner("Exporting model summary..."):
                    model_info = {
                        "model_object": training_info['model'],
                        "model_name": training_info['model_name'],
                        "X_train": training_info['X_train'],
                        "X_test": training_info['X_test'],
                        "y_train": training_info['y_train'],
                        "y_test": training_info['y_test'],
                        "params": training_info['params'],
                        "is_supervised": True
                    }
                    
                    summary = export_model_summary(model_info, evaluation_results)
                    
                    import json
                    summary_json = json.dumps(summary, indent=2, default=str)
                    
                    summary_filename = f"model_summary_{st.session_state.model_filename}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                    
                    st.download_button(
                        label="â¬‡ï¸ Download Model Summary",
                        data=summary_json,
                        file_name=summary_filename,
                        mime="application/json",
                        use_container_width=True
                    )
                    
                    st.success("âœ… Model summary exported successfully!")
                    st.info(f"ğŸ“ **Summary:** {summary_filename}")
                    st.info(f"ğŸ“Š **Size:** {len(summary_json.encode()) / 1024:.1f} KB")
                
            except Exception as e:
                st.error(f"âŒ Error exporting summary: {str(e)}")
    
    st.markdown("---")
    st.subheader("ğŸš€ Deployment Instructions")
    
    deployment_tab1, deployment_tab2, deployment_tab3 = st.tabs([
        "ğŸ Python Usage", 
        "ğŸŒ Web Deployment",
        "â˜ï¸ Cloud Deployment"
    ])
    
    with deployment_tab1:
        st.write("**ğŸ“ Python Code Example:**")
        
        feature_list = "', '".join(training_info['feature_columns'])
        
        code_example = f"""
# Load and use your trained model
import joblib
import pandas as pd
import numpy as np

# Load the model package
model_package = joblib.load('{st.session_state.model_filename}.{export_format}')
model = model_package['model']
label_encoder = model_package.get('label_encoder')  # May be None for regression

# Prepare your data (ensure same features and order)
required_features = ['{feature_list}']

# Example data (replace with your actual data)
new_data = pd.DataFrame({{
    # Add your data here with the same column names
    # Example:
    # '{training_info['feature_columns'][0]}': [value1, value2, ...],
    # Continue for all features...
}})

# Make predictions
predictions = model.predict(new_data)

# For classification models, convert back to original labels
if label_encoder is not None:
    predictions = label_encoder.inverse_transform(predictions)

# For classification models with probability estimates:
if hasattr(model, 'predict_proba'):
    probabilities = model.predict_proba(new_data)
    
print("Predictions:", predictions)
"""
        
        st.code(code_example, language='python')
    
    with deployment_tab2:
        st.write("**ğŸŒ Streamlit Web App Template:**")
        
        streamlit_code = f"""
import streamlit as st
import joblib
import pandas as pd

# Load model
@st.cache_resource
def load_model():
    model_package = joblib.load('{st.session_state.model_filename}.{export_format}')
    return model_package['model'], model_package.get('label_encoder')

model, label_encoder = load_model()

st.title('ML Model Prediction App')

# Create input fields for each feature
feature_inputs = {{}}
"""
        
        for feature in training_info['feature_columns'][:5]:  # Show first 5 features
            streamlit_code += f"""
feature_inputs['{feature}'] = st.number_input('{feature}', value=0.0)
"""
        
        streamlit_code += """
# Make prediction
if st.button('Predict'):
    input_data = pd.DataFrame([feature_inputs])
    prediction = model.predict(input_data)
    
    # Convert back to original labels if needed
    if label_encoder is not None:
        prediction = label_encoder.inverse_transform(prediction)
    
    st.success(f'Prediction: {prediction[0]}')
"""
        
        st.code(streamlit_code, language='python')
    
    with deployment_tab3:
        st.write("**â˜ï¸ Cloud Deployment Options:**")
        
        st.markdown("""
        **Recommended Cloud Platforms:**
        
        1. **Streamlit Cloud** (Free)
           - Upload your code to GitHub
           - Connect to Streamlit Cloud
           - Automatic deployment
        
        2. **Heroku** 
           - Create Procfile and requirements.txt
           - Deploy via Git or GitHub
           - Add-ons available for databases
        
        3. **AWS/Google Cloud/Azure**
           - Use container services (ECS, Cloud Run, Container Instances)
           - Serverless functions (Lambda, Cloud Functions, Azure Functions)
           - ML-specific services (SageMaker, AI Platform, ML Studio)
        
        4. **Docker Deployment**
           - Create Dockerfile for your app
           - Deploy to any container platform
           - Kubernetes for scaling
        """)
        
        st.write("**ğŸ³ Docker Example:**")
        docker_code = f"""
# Dockerfile
FROM python:3.9-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .
EXPOSE 8501

CMD ["streamlit", "run", "app.py", "--server.port=8501", "--server.address=0.0.0.0"]

# Build and run:
# docker build -t ml-model-app .
# docker run -p 8501:8501 ml-model-app
"""
        st.code(docker_code, language='dockerfile')

def clean_numbers(df):
    """
    Cleans Indonesian number formatting in a Pandas DataFrame.
    This function replaces dots ('.') used as thousand separators with empty strings,
    and commas (',') used as decimal points with dots ('.').

    Args:
        df (pd.DataFrame): The input DataFrame.

    Returns:
        pd.DataFrame: The cleaned DataFrame.
    """
    for col in df.columns:
        if df[col].dtype == 'object':
            try:
                # Clean Indonesian number format
                cleaned = df[col].str.replace('\.', '', regex=True).str.replace(',', '.', regex=False).astype(float)
                df[col] = cleaned
            except:
                # If conversion fails, leave as is
                pass
    return df
